# Security Review of [ollama_bash_lib.sh](../ollama_bash_lib.sh)

A [demo](../README.md#demos) of [Ollama Bash Lib](https://github.com/attogram/ollama-bash-lib) v0.42.35


```bash
task="Act as an Expert Security Professional.
Do a Security Review of this bash script.
Require that script works in Bash v3 or higher.
Output your review in Markdown format."
file="../ollama_bash_lib.sh"
ollama_generate "gpt-oss:20b" "$task\n\n$(cat "$file")"
```
# Security Review – `ollama-bash-lib` (Bash v3+)

> **Target:** An opinionated Bash library that talks to the Ollama API, generates LLM completions, and (optionally) evaluates the produced code.  
> **Scope:** All internal functions, public helpers, global variables, and potential injection paths.  
> **Assumptions:** The script runs on a Linux‑style OS with the standard GNU utilities, `bash` ≥ 3.2, `curl` and `jq` installed.  
> **Impact:** The library primarily handles *trusted* user input (the LLM prompt).  When the LLM is untrusted, several paths can lead to remote code execution (RCE), disclosure of secrets, or denial‑of‑service (DoS).

---

## 1. Global Configuration & Secret Handling

| Issue | Why it matters | Suggested fix |
|-------|----------------|---------------|
| **Plain text storage of `OLLAMA_LIB_TURBO_KEY`** | The key is kept in a shell variable that lives in the process’ memory.  Any child process or a memory‑dump attacker could recover it. | Add `export` only when needed, otherwise keep it in a temporary file and `unset` after use. Use `read -r -s` into a local variable and `unset` immediately instead of a global var. |
| **Debug logs reveal API headers & body** | Debug prints `"$json_body"` (first 120 chars) and the entire URL. If `OLLAMA_LIB_DEBUG` is enabled on a public machine, secrets can leak. | Disable debug by default. When enabled, filter request bodies for `Authorization` or API keys. |
| **Environment variable `OLLAMA_HOST` is user‑controlled** | Whoever can set `OLLAMA_HOST` can point the library to an arbitrary HTTP endpoint and cause e.g. a man‑in‑the‑middle or phishing server to receive LLM data. | Verify that `OLLAMA_HOST` matches the pattern `http[s]://[^/]+:?[0-9]*` before using it. In strict mode refuse to proceed if the host is untrusted. |
| **No TTL or expiration on `OLLAMA_LIB_TURBO_KEY`** | A script that stores the key may never clear it, leading to accidental export to other scripts. | Implement a `clear_turbo_key` helper that `unset`’s the variable and add a guard to `ollama_app_turbo`. |
| **Variable re‑usage** | `OLLAMA_LIB_API` is derived from `OLLAMA_HOST`, but the host can be changed after the script source is read, leading to inconsistencies. | Compute `OLLAMA_LIB_API` *inside* each function that needs it, or keep it `readonly`. |

---

## 2. External Command Availability

| Problem | Impact | Mitigation |
|---------|--------|------------|
| Functions rely on `curl`, `jq`, `awk`, `sed`, `head`, `sort`, `read`, `column` but never verify existence before first use. | If any dep is missing, the command will explode at runtime (usually echoing a non‑zero status). In the worst case, subsequent logic may be incorrectly assumed successful. | Add a one‑time check at library load:

```bash
for cmd in curl jq awk sed head sort column; do
  if ! _exists "$cmd"; then
    _error "Missing required command: $cmd"
    exit 1
  fi
done
```

|

---

## 3. Argument Sanitisation & Injection Points

| Function | Injection surface | Observed sanitisation | Potential risk | Recommendation |
|---------|------------------|----------------------|----------------|----------------|
| `_call_curl` | `endpoint`, `method`, `json_body` | `curl` arguments passed as an array – safe from word‑splitting. | `endpoint` may contain spaces or shell metacharacters. If a user passes `" /bad; rm -rf /"` the library will *trivially* send a request to `http://localhost:11434 /bad; rm -rf /`.  The server will ignore `;` but the request could still be malformed. | Validate that `endpoint` starts with `/` and contains only `A-Za-z0-9/_-.`. Reject otherwise. |
| `ollama_generate_json`, `ollama_generate` | Prompt (`$2`) is passed to the LLM *verbatim*; the LLM could return arbitrary JSON. | The API body is constructed via `jq -c -n` and `--arg`, so no shell injection from that side. | Validate any JSON returned by the API with `jq` before processing. If `jq` fails, treat it as an error and abort. |
| `ollama_eval` | The *command* generated by the LLM (`$cmd`) is directly passed to `eval`. | The function prompts the user to confirm execution, but an attacker can circumvent the prompt by automating the script or by tricking the user into answering “y”. | **DO NOT** use `eval` on untrusted output. Either 1) refuse to execute automatic commands, or 2) run the command in a sandbox (`bash -c` with restricted `PATH` and `ulimit`), or 3) expose a separate, controlled interpreter (e.g., `python -c`, `node -e`) that can be more tightly scoped. |
| `ollama_app_turbo` | The user-provided API key is echoed back to the terminal (`echo $api_key`). | The key is stored in a variable visible to processes that share the process tree. | Do not echo the key. Write it to a temporary file with `chmod 600` and delete afterwards. |

---

## 4. Safe‑Mode & Control‑character Escaping

| Feature | Concern | Fix |
|---------|---------|-----|
| `_escape_control_characters` operates only when `OLLAMA_LIB_SAFE_MODE=1`. | An attacker can enable safe mode off and inject raw control characters (e.g., `\b`, `\n`, `\0`) into the JSON, causing downstream parsers (`jq`) to error or mis‑interpret data. | Default to `SAFE_MODE=1`; document the risk of turning it off and restrict toggling to privileged environments. |
| The function currently prints debug fragments (`$1:0:120`). A malicious prompt containing a large block may overflow the debug buffer or conceal injection. | Not a security flaw per se, but debug messages could produce large logs. | Limit debug length to a smaller amount or log to separate file with appropriate rotation. |
| Escape logic may inadvertently double‑escape existing `\u` sequences in the model output, corrupting valid JSON. | No immediate security risk, but could lead to false negatives during validation. | Use `jq`'s `ascii_downcase` or rely on `jq -c .` to normalise JSON. |

---

## 5. Bash‑3 Compatibility & Potential Runtime Issues

| Check | Why it matters | Evidence in script |
|-------|----------------|-------------------|
| Substring expansion `${var:0:N}` | Requires Bash 3.2+. | The script uses it everywhere – safe on Bash ≥ 3.2. |
| Process substitution `<(...)` | Available since Bash 3.2. | Used in many functions – safe. |
| `local` inside functions | Works in Bash 3.2+. | All functions use `local`. |
| `printf '%b'` for octal expansion | `printf` supports `%b` since early releases. | Correct. |
| `tr -s ' ' '\n'` | Works on POSIX systems. | Safe. |

**No major compatibility issues.** However, the script assumes `date +%N` (nanoseconds) and GNU `date`. On BSD/macOS `date +%N` is unsupported – it will expand literally to `+%N`. The debug output may therefore leak the literal string and be less sensitive. To be fully portable, replace with `date +%s%N || date +%s` fallback.

---

## 6. Concurrency & Global State

The library stores conversation history in the global array `OLLAMA_LIB_MESSAGES` and state variables `OLLAMA_LIB_STREAM`, `OLLAMA_LIB_SAFE_MODE`, `OLLAMA_LIB_TURBO_KEY`. **If multiple instances of the script are run concurrently in the same shell session (e.g., sourced from multiple files), they will share and potentially corrupt each other's state.**  

*Mitigations:*

- Prefix all state variables with a unique namespace (e.g., `OLOBLA_`) or make them `readonly` after initialization.
- Provide a `ollama_lib_reset` helper that clears all globals.

---

## 7. Denial‑of‑Service & Resource Exhaustion

| Edge | Why | Fix |
|------|-----|-----|
| `ollama_generate_json` & `ollama_generate` read entire response via `$(...)`. | If the LLM returns an extremely large model, the process will consume memory or may hang. | Stream the response (`curl -N | jq .`) or limit size with `head -c`. |
| `ollama_eval` runs the returned command *verbatim*. | An attacker may produce a long-running or recursive script causing the system to exhaust CPU or memory. | Enforce a timeout (`timeout` command) when executing. |
| `ollama_list`, `ollama_ps`, `ollama_show` use `awk`/`sed` without limiting input size. | If `ollama list` prints a huge output, it could flood stdout. | Pipe output through `head -n 1000` or similar. |

---

## 8. Logging & Error Handling

| Issue | Impact | Fix |
|-------|--------|-----|
| Some functions swallow errors (e.g., `ollama_list`) by checking `$?` but not returning on failure. | The user may think the command succeeded while data is omitted. | Return non‑zero status and propagate error message. |
| No `set -e`, `set -u` or `set -o pipefail`. | Script may silently ignore errors in pipes (e.g., `head` failing after `curl`). | Enable strict mode at the top:

```bash
set -euo pipefail
```

Then use `export PATH` carefully. |

---

## 9. Recommendations & Security Checklist

| What to do | Where to implement |
|------------|--------------------|
| **Validate user‑supplied endpoints & methods.** Reject anything that is not a valid sub‑path and only allow `GET`/`POST`. | `_call_curl` |
| **Force `jq` and `curl` presence at load time.** | At script start (before any function definitions) |
| **Remove `eval` or change to safe execution.** | `ollama_eval` – use `bash -c "$cmd"` with a restricted `PATH` and `ulimit` |
| **Make safe‑mode the default.** | Set `readonly OLLAMA_LIB_SAFE_MODE=1` |
| **Strip debug output in production.** | `OLLAMA_LIB_DEBUG=0` by default; optional environment guard |
| **Use temporary files for API keys.** | `ollama_app_turbo` – write key to `/tmp/ollama_key_$$` with `chmod 600`; `unset` after usage |
| **Add input length limits.** | In `ollama_list` and `ollama_ps`, limit lines read from stdout. |
| **Implement timeout for network requests.** | Add `--max-time` to `curl`, and `--retry`/`--retry-all-errors` if desired |
| **Sanity‑check JSON responses.** | After `ollama_api_get` and `ollama_api_post`, run `jq .` and exit on failure |
| **Separate sandbox environment.** | If user allows code execution, drop to a restricted user or container. |
| **Document all globals and their lifecycle.** | Add a `# GLOBALS:` section in the header. |

---

## 10. Final Verdict

*Overall, the library is a useful tool for interacting with Ollama but it is **not** safe to use in untrusted or production environments *without* the following mitigations:*

1. **Input validation** for all external endpoints.  
2. **Explicit dependency checks**.  
3. **Hard‑coded safe mode**.  
4. **Secure handling of API keys** (no echo, no long‑term storage).  
5. **Removal or sandboxing of `eval`**.  
6. **Appropriate error handling & timeouts** to avoid DoS.  

Implementing the above changes will bring the library to a level that an end user can confidently embed in scripts that may be run by untrusted parties.
